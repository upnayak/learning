{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIL NOUN compound\n",
      "foreigners NOUN nsubjpass\n",
      "that ADJ nsubj\n",
      "owe VERB relcl\n",
      "more ADJ amod\n",
      "than ADP quantmod\n",
      "£ SYM quantmod\n",
      "1,000 NUM dobj\n",
      "to ADP prep\n",
      "the DET det\n",
      "NHS PROPN pobj\n",
      "are VERB auxpass\n",
      "banned VERB ROOT\n",
      "from ADP prep\n",
      "entry NOUN pobj\n",
      "into ADP prep\n",
      "the DET det\n",
      "United PROPN compound\n",
      "Kingdom PROPN pobj\n"
     ]
    }
   ],
   "source": [
    "#Create a DOC object\n",
    "doc = nlp(u\"TIL foreigners that owe more than £1,000 to the NHS are banned from entry into the United Kingdom\")\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Tokenization\n",
    "The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Doc object to contain descriptive information.\n",
    "\n",
    "## 2. Part-of-Speech Tagging (POS)\n",
    "The next step after splitting the text up into tokens is to assign parts of speech. In the above example, `NHS` was recognized to be a ***proper noun***. Here some statistical modeling is required. For example, words that follow \"the\" are typically nouns.\n",
    "\n",
    "For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging\n",
    "\n",
    "## 3. Dependencies\n",
    "We also looked at the syntactic dependencies assigned to each token. `foreigners` is identified as an `nsubjpass` or the ***nominal subject*** of the sentence.\n",
    "\n",
    "For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n",
    "<br>A good explanation of typed dependencies can be found [here](https://nlp.stanford.edu/software/dependencies_manual.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Additional Token Attributes\n",
    "We'll see these again in upcoming lectures. For now we just want to illustrate some of the other information that spaCy assigns to tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Tag|Description|doc2[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n",
    "|`.lemma_`|The base form of the word|`tesla`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prepositional modifier'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banned\n",
      "ban\n"
     ]
    }
   ],
   "source": [
    "# Lemmas (the base form of the word):\n",
    "print(doc[12].text)\n",
    "print(doc[12].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      "ADP\n",
      "IN / conjunction, subordinating or preposition\n"
     ]
    }
   ],
   "source": [
    "# Simple Parts-of-Speech & Detailed Tags:\n",
    "print(doc[13].text)\n",
    "print(doc[13].pos_)\n",
    "print(doc[13].tag_ + ' / ' + spacy.explain(doc[13].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more: d,ddd\n"
     ]
    }
   ],
   "source": [
    "# Word Shapes:\n",
    "print(doc[7].text+': '+doc[7].shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "£ - False\n",
      "more - True\n"
     ]
    }
   ],
   "source": [
    "# Boolean Values:\n",
    "print(f\"{doc[6].text} - {doc[6].is_alpha}\")\n",
    "print(f\"{doc[4].text} - {doc[4].is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Doc.sents`. Later we'll write our own segmentation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: It's not very bad for wild populations of birds since most species are natural carriers.\n",
      "Sentence 1: The problem is with domestic birds such a as chickens cause they are a lot more vulnerable to the bird flu.\n",
      "Sentence 2: At least that's what they told us in vet school.\n",
      "Sentence 3: They also told us to keep chickens separated from other bird species such as turkeys for example, cause they can also be carriers of the virus.\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"It's not very bad for wild populations of birds since most species are natural carriers. The problem is with domestic birds such a as chickens cause they are a lot more vulnerable to the bird flu. At least that's what they told us in vet school. They also told us to keep chickens separated from other bird species such as turkeys for example, cause they can also be carriers of the virus.\")\n",
    "for i,sent in enumerate(doc2.sents):\n",
    "    print(f\"Sentence {i}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
